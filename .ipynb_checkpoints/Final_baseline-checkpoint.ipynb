{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done makeing vocab\n",
      "Done loading embedding\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of words in vocab\n",
    "vocab = {}\n",
    "with open('glove.6B.100d.txt' , 'r') as inpfile:\n",
    "    line = inpfile.readline()\n",
    "    while line:\n",
    "        line = line.strip()\n",
    "        line = line.split()\n",
    "        vocab[line[0]] = 1\n",
    "        line = inpfile.readline()\n",
    "print('Done making vocab')\n",
    "\n",
    "#load word2vec embedding\n",
    "\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "print ('Done loading embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returns cosine similartiy between given words\n",
    "def calculate_cosine_similartiy(word1 , word2):\n",
    "    A = model[word1 ]\n",
    "    A = np.reshape(A, (1, -1))\n",
    "    B = model[word2 ]\n",
    "    B = np.reshape(B, (1, -1))\n",
    "    x = cosine_similarity( A , B )[0][0]\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e4afbca82ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.txt'"
     ]
    }
   ],
   "source": [
    "#convert training data in representational format\n",
    "\n",
    "cosine_similarity_words_features = []  \n",
    "concatinated_word_embeddings     = []\n",
    "label = []\n",
    "\n",
    "\n",
    "total = 0   #total rows in train data\n",
    "cnt   = 0   #points present in vocab\n",
    "output_file_one  = open('train_output_one.txt', 'w')\n",
    "output_file_zero = open('train_output_zero.txt', 'w')\n",
    "\n",
    "\n",
    "with open('train.txt') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    for row in reader :\n",
    "        total += 1\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            cnt += 1\n",
    "            continue\n",
    "        cosine_similarity_between_w1_atb = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        cosine_similarity_between_w2_atb = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        #print(cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb)\n",
    "        if (row[3]=='1'):\n",
    "            output_file_one.write(row[0]+\" \" + row[1]+\" \" + row[2] +'\\n' )\n",
    "            output_file_one.write(str(cosine_similarity_between_w1_atb)+\" \" + str(cosine_similarity_between_w2_atb)+\" \" + str( row[3] )+'\\n' )\n",
    "        else:\n",
    "            output_file_zero.write(row[0]+\" \" + row[1]+\" \" + row[2] +'\\n' )\n",
    "            output_file_zero.write(str(cosine_similarity_between_w1_atb)+\" \" + str(cosine_similarity_between_w2_atb)+\" \" + str( row[3] )+'\\n' )\n",
    "        \n",
    "        label.append([int(row[3])])\n",
    "        cosine_similarity_words_features.append([cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb ])\n",
    "        concatinated_list = (model[row[0]]  +  model[row[1]]  + model[row[2]] ).tolist()\n",
    "        concatinated_word_embeddings.append(concatinated_list  )\n",
    "        \n",
    "print('Done representing')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting\n"
     ]
    }
   ],
   "source": [
    "#train logistic regression model by using cosine values as feature vector\n",
    "\n",
    "logistic_regr_model_cosine_sim   = LogisticRegression(max_iter = 300)\n",
    "logistic_regr_model_cosine_sim.fit( cosine_similarity_words_features , label )\n",
    "\n",
    "print(\"Done fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting\n"
     ]
    }
   ],
   "source": [
    "#train logistic regression model by concatinating word embedding values as feature vector\n",
    "\n",
    "\n",
    "logistic_regr_model_concatinated = LogisticRegression(max_iter = 300)\n",
    "logistic_regr_model_concatinated.fit( concatinated_word_embeddings , label  ) \n",
    "print(\"Done fitting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1341 2721 0.4928335170893054\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using logistic regression model using cosine values as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        \n",
    "        cosine_similarity_between_w1_atb = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        cosine_similarity_between_w2_atb = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        \n",
    "        predicted_class = logistic_regr_model_cosine_sim.predict( [cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb] )\n",
    "        \n",
    "        #print(predicted_class , row[3])\n",
    "        \n",
    "        if(predicted_class[0] == int(row[3])):\n",
    "            cnt += 1\n",
    "print (cnt , total , cnt/total)\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1354 2721 0.4976111723631018\n",
      "Done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using logistic regression model using concatinating embedding as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        concatinated_list = (model[row[0]]  +  model[row[1]]  + model[row[2]] ).tolist()\n",
    "        predicted_class = logistic_regr_model_concatinated.predict(concatinated_list)\n",
    "        \n",
    "        #print( predicted_class , row[3])\n",
    "        if(predicted_class[0]==int(row[3])):\n",
    "            cnt += 1\n",
    "print( cnt , total , cnt/total)\n",
    "print('Done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done model fitting\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Naive bayes using cosine_values as embeddings\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "gnb_cosine = GaussianNB()\n",
    "NB_model_cosine = gnb_cosine.fit(cosine_similarity_words_features , label)\n",
    "gnb_concat = GaussianNB()\n",
    "NB_model_concat = gnb_concat.fit(concatinated_word_embeddings , label)\n",
    "\n",
    "print(\"Done model fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242 2721 0.4564498346196251\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using Naive Baise model using cosine values as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        \n",
    "        cosine_similarity_between_w1_atb = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        cosine_similarity_between_w2_atb = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        \n",
    "        predicted_class = NB_model_cosine.predict( [cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb] )\n",
    "        \n",
    "        #print(predicted_class , row[3])\n",
    "        \n",
    "        if(predicted_class[0] == int(row[3])):\n",
    "            cnt += 1\n",
    "print (cnt , total , cnt/total)\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223 2721 0.4494671076809996\n",
      "Done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using Naive Bayes model using concatinating embedding as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        concatinated_list = (model[row[0]]  +  model[row[1]]  + model[row[2]] ).tolist()\n",
    "        predicted_class = NB_model_concat.predict(concatinated_list)\n",
    "        \n",
    "        #print( predicted_class , row[3])\n",
    "        if(predicted_class[0]==int(row[3])):\n",
    "            cnt += 1\n",
    "print( cnt , total , cnt/total)\n",
    "print('Done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
