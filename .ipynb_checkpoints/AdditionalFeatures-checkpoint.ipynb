{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making vocab\n",
      "Done loading embedding\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of words in vocab\n",
    "vocab = {}\n",
    "with open('glove.6B.100d.txt' , 'r') as inpfile:\n",
    "    line = inpfile.readline()\n",
    "    while line:\n",
    "        line = line.strip()\n",
    "        line = line.split()\n",
    "        vocab[line[0]] = 1\n",
    "        line = inpfile.readline()\n",
    "print('Done making vocab')\n",
    "\n",
    "#load word2vec embedding\n",
    "\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "print ('Done loading embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returns cosine similartiy between given words\n",
    "def calculate_cosine_similartiy(word1 , word2):\n",
    "    A = model[word1 ]\n",
    "    A = np.reshape(A, (1, -1))\n",
    "    B = model[word2 ]\n",
    "    B = np.reshape(B, (1, -1))\n",
    "    x = cosine_similarity( A , B )[0][0]\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done representing\n"
     ]
    }
   ],
   "source": [
    "#convert training data in representational format\n",
    "\n",
    "cosine_similarity_words_features = []  \n",
    "concatinated_word_embeddings     = []\n",
    "label = []\n",
    "\n",
    "\n",
    "total = 0   #total rows in train data\n",
    "cnt   = 0   #points present in vocab\n",
    "output_file_one  = open('train_output_one.txt', 'w')\n",
    "output_file_zero = open('train_output_zero.txt', 'w')\n",
    "\n",
    "\n",
    "with open('train.txt') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    for row in reader :\n",
    "        total += 1\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            cnt += 1\n",
    "            continue\n",
    "        cosine_similarity_between_w1_atb = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        cosine_similarity_between_w2_atb = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        #print(cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb)\n",
    "        if (row[3]=='1'):\n",
    "            output_file_one.write(row[0]+\" \" + row[1]+\" \" + row[2] +'\\n' )\n",
    "            output_file_one.write(str(cosine_similarity_between_w1_atb)+\" \" + str(cosine_similarity_between_w2_atb)+\" \" + str( row[3] )+'\\n' )\n",
    "        else:\n",
    "            output_file_zero.write(row[0]+\" \" + row[1]+\" \" + row[2] +'\\n' )\n",
    "            output_file_zero.write(str(cosine_similarity_between_w1_atb)+\" \" + str(cosine_similarity_between_w2_atb)+\" \" + str( row[3] )+'\\n' )\n",
    "        \n",
    "        label.append([int(row[3])])\n",
    "        cosine_similarity_words_features.append([cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb ])\n",
    "        concatinated_list = (model[row[0]]  +  model[row[1]]  + model[row[2]] ).tolist()\n",
    "        concatinated_word_embeddings.append(concatinated_list  )\n",
    "        \n",
    "print('Done representing')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting\n"
     ]
    }
   ],
   "source": [
    "#train logistic regression model by using cosine values as feature vector\n",
    "\n",
    "logistic_regr_model_cosine_sim   = LogisticRegression(max_iter = 300)\n",
    "logistic_regr_model_cosine_sim.fit( cosine_similarity_words_features , label )\n",
    "\n",
    "print(\"Done fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting\n"
     ]
    }
   ],
   "source": [
    "#train logistic regression model by concatinating word embedding values as feature vector\n",
    "\n",
    "\n",
    "logistic_regr_model_concatinated = LogisticRegression(max_iter = 300)\n",
    "logistic_regr_model_concatinated.fit( concatinated_word_embeddings , label  ) \n",
    "print(\"Done fitting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1341 2721 0.4928335170893054\n",
      "precision =  0.49402670414617006  recall =  0.5157740278796772\n",
      "F1 score =  0.5046661880832735\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using logistic regression model using cosine values as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "true_positive  = 0\n",
    "true_negative  = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        \n",
    "        cosine_similarity_between_w1_atb = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        cosine_similarity_between_w2_atb = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        \n",
    "        predicted_class = logistic_regr_model_cosine_sim.predict( [cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb] )\n",
    "        \n",
    "        #print(predicted_class , row[3])\n",
    "        \n",
    "        if(predicted_class[0] == int(row[3])):\n",
    "            cnt += 1\n",
    "            \n",
    "        if(int(row[3]) == 1):\n",
    "            if(predicted_class[0]==1):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        else:\n",
    "            if(predicted_class[0]==1):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        \n",
    "precision = true_positive/( true_positive + false_positive)\n",
    "recall    = true_positive/( true_positive + false_negative)\n",
    "            \n",
    "print (cnt , total , cnt/total)\n",
    "print ('precision = ' , precision , ' recall = ' , recall )\n",
    "print ( 'F1 score = ' , 2*((precision*recall)/ (precision+recall))  )\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1354 2721 0.4976111723631018\n",
      "precision =  0.4978021978021978  recall =  0.33235509904622157\n",
      "F1 score =  0.3985921689397272\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using logistic regression model using concatinating embedding as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "true_positive  = 0\n",
    "true_negative  = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        concatinated_list = (model[row[0]]  +  model[row[1]]  + model[row[2]] ).tolist()\n",
    "        predicted_class = logistic_regr_model_concatinated.predict(concatinated_list)\n",
    "        \n",
    "        #print( predicted_class , row[3])\n",
    "        if(predicted_class[0]==int(row[3])):\n",
    "            cnt += 1\n",
    "        if(int(row[3]) == 1):\n",
    "            if(predicted_class[0]==1):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        else:\n",
    "            if(predicted_class[0]==1):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        \n",
    "precision = true_positive/( true_positive + false_positive)\n",
    "recall    = true_positive/( true_positive + false_negative)\n",
    "            \n",
    "print (cnt , total , cnt/total)\n",
    "print ('precision = ' , precision , ' recall = ' , recall )\n",
    "print ( 'F1 score = ' , 2*((precision*recall)/ (precision+recall))  )\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done model fitting\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Naive bayes using cosine_values as embeddings\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "gnb_cosine = GaussianNB()\n",
    "NB_model_cosine = gnb_cosine.fit(cosine_similarity_words_features , label)\n",
    "gnb_concat = GaussianNB()\n",
    "NB_model_concat = gnb_concat.fit(concatinated_word_embeddings , label)\n",
    "\n",
    "print(\"Done model fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242 2721 0.4564498346196251\n",
      "precision =  0.46432964329643295  recall =  0.5539251650770359\n",
      "F1 score =  0.5051856808297089\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using Naive Baise model using cosine values as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "true_positive  = 0\n",
    "true_negative  = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        \n",
    "        cosine_similarity_between_w1_atb = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        cosine_similarity_between_w2_atb = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        \n",
    "        predicted_class = NB_model_cosine.predict( [cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb] )\n",
    "        \n",
    "        #print(predicted_class , row[3])\n",
    "        \n",
    "        if(predicted_class[0] == int(row[3])):\n",
    "            cnt += 1\n",
    "        if(int(row[3]) == 1):\n",
    "            if(predicted_class[0]==1):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        else:\n",
    "            if(predicted_class[0]==1):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        \n",
    "precision = true_positive/( true_positive + false_positive)\n",
    "recall    = true_positive/( true_positive + false_negative)\n",
    "            \n",
    "print (cnt , total , cnt/total)\n",
    "print ('precision = ', precision , ' recall = ' , recall )\n",
    "print ( 'F1 score = ' , 2*((precision*recall)/ (precision+recall))  )\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223 2721 0.4494671076809996\n",
      "precision =  0.4432296047098402  recall =  0.38664710198092445\n",
      "F1 score =  0.41300940438871475\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using Naive Bayes model using concatinating embedding as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "total =0\n",
    "cnt = 0\n",
    "true_positive  = 0\n",
    "true_negative  = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        concatinated_list = (model[row[0]]  +  model[row[1]]  + model[row[2]] ).tolist()\n",
    "        predicted_class = NB_model_concat.predict(concatinated_list)\n",
    "        \n",
    "        #print( predicted_class , row[3])\n",
    "        if(predicted_class[0]==int(row[3])):\n",
    "            cnt += 1\n",
    "        \n",
    "        if(int(row[3]) == 1):\n",
    "            if(predicted_class[0]==1):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        else:\n",
    "            if(predicted_class[0]==1):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        \n",
    "precision = true_positive/( true_positive + false_positive)\n",
    "recall    = true_positive/( true_positive + false_negative)\n",
    "            \n",
    "print (cnt , total , cnt/total)\n",
    "print ('precision = ',  precision , ' recall = ' ,recall )\n",
    "print ( 'F1 score = ' , 2*((precision*recall)/ (precision+recall))  )\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional feature extraction using wordnet\n",
    "\n",
    "with open('train.txt') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    for row in reader :\n",
    "        total += 1\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            cnt += 1\n",
    "            continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
