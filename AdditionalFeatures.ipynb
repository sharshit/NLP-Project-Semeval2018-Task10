{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making vocab\n",
      "Done loading embedding\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of words in vocab\n",
    "vocab = {}\n",
    "with open('glove.6B.300d.txt' , 'r') as inpfile:\n",
    "    line = inpfile.readline()\n",
    "    while line:\n",
    "        line = line.strip()\n",
    "        line = line.split()\n",
    "        vocab[line[0]] = 1\n",
    "        line = inpfile.readline()\n",
    "print('Done making vocab')\n",
    "\n",
    "#load word2vec embedding\n",
    "\n",
    "glove_input_file = 'glove.6B.300d.txt'\n",
    "word2vec_output_file = 'glove.6B.300d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "filename = 'glove.6B.300d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "print ('Done loading embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returns cosine similartiy between given words\n",
    "def calculate_cosine_similartiy(word1 , word2):\n",
    "    A = model[word1 ]\n",
    "    A = np.reshape(A, (1, -1))\n",
    "    B = model[word2 ]\n",
    "    B = np.reshape(B, (1, -1))\n",
    "    x = cosine_similarity( A , B )[0][0]\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done representing\n"
     ]
    }
   ],
   "source": [
    "#convert training data in representational format\n",
    "\n",
    "cosine_similarity_words_features = []  \n",
    "concatinated_word_embeddings     = []\n",
    "label = []\n",
    "\n",
    "\n",
    "total = 0   #total rows in train data\n",
    "cnt   = 0   #points present in vocab\n",
    "output_file_one  = open('train_output_one.txt', 'w')\n",
    "output_file_zero = open('train_output_zero.txt', 'w')\n",
    "\n",
    "\n",
    "with open('train.txt') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    for row in reader :\n",
    "        total += 1\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            cnt += 1\n",
    "            continue\n",
    "        cosine_similarity_between_w1_atb = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        cosine_similarity_between_w2_atb = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        #print(cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb)\n",
    "        if (row[3]=='1'):\n",
    "            output_file_one.write(row[0]+\" \" + row[1]+\" \" + row[2] +'\\n' )\n",
    "            output_file_one.write(str(cosine_similarity_between_w1_atb)+\" \" + str(cosine_similarity_between_w2_atb)+\" \" + str( row[3] )+'\\n' )\n",
    "        else:\n",
    "            output_file_zero.write(row[0]+\" \" + row[1]+\" \" + row[2] +'\\n' )\n",
    "            output_file_zero.write(str(cosine_similarity_between_w1_atb)+\" \" + str(cosine_similarity_between_w2_atb)+\" \" + str( row[3] )+'\\n' )\n",
    "        \n",
    "        label.append([int(row[3])])\n",
    "        cosine_similarity_words_features.append([cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb ])\n",
    "        concatinated_list = (model[row[0]]  +  model[row[1]]  + model[row[2]] ).tolist()\n",
    "        concatinated_word_embeddings.append(concatinated_list  )\n",
    "        \n",
    "print('Done representing')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting\n"
     ]
    }
   ],
   "source": [
    "#train logistic regression model by using cosine values as feature vector\n",
    "\n",
    "logistic_regr_model_cosine_sim   = LogisticRegression()\n",
    "logistic_regr_model_cosine_sim.fit( cosine_similarity_words_features , label )\n",
    "\n",
    "print(\"Done fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting\n"
     ]
    }
   ],
   "source": [
    "#train logistic regression model by concatinating word embedding values as feature vector\n",
    "\n",
    "\n",
    "logistic_regr_model_concatinated = LogisticRegression(max_iter = 300)\n",
    "logistic_regr_model_concatinated.fit( concatinated_word_embeddings , label  ) \n",
    "print(\"Done fitting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1397 2721 0.5134141859610437\n",
      "precision =  0.5144337527757217  recall =  0.5099046221570066\n",
      "F1 score =  0.5121591746499632\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using logistic regression model using cosine values as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "true_positive  = 0\n",
    "true_negative  = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        \n",
    "        cosine_similarity_between_w1_atb = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        cosine_similarity_between_w2_atb = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        \n",
    "        predicted_class = logistic_regr_model_cosine_sim.predict( [cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb] )\n",
    "        \n",
    "        #print(predicted_class , row[3])\n",
    "        \n",
    "        if(predicted_class[0] == int(row[3])):\n",
    "            cnt += 1\n",
    "            \n",
    "        if(int(row[3]) == 1):\n",
    "            if(predicted_class[0]==1):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        else:\n",
    "            if(predicted_class[0]==1):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        \n",
    "precision = true_positive/( true_positive + false_positive)\n",
    "recall    = true_positive/( true_positive + false_negative)\n",
    "            \n",
    "print (cnt , total , cnt/total)\n",
    "print ('precision = ' , precision , ' recall = ' , recall )\n",
    "print ( 'F1 score = ' , 2*((precision*recall)/ (precision+recall))  )\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407 2721 0.5170893054024256\n",
      "precision =  0.5248730964467005  recall =  0.3793103448275862\n",
      "F1 score =  0.4403747870528109\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using logistic regression model using concatinating embedding as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "true_positive  = 0\n",
    "true_negative  = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        concatinated_list = (model[row[0]]  +  model[row[1]]  + model[row[2]] ).tolist()\n",
    "        predicted_class = logistic_regr_model_concatinated.predict(concatinated_list)\n",
    "        \n",
    "        #print( predicted_class , row[3])\n",
    "        if(predicted_class[0]==int(row[3])):\n",
    "            cnt += 1\n",
    "        if(int(row[3]) == 1):\n",
    "            if(predicted_class[0]==1):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        else:\n",
    "            if(predicted_class[0]==1):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        \n",
    "precision = true_positive/( true_positive + false_positive)\n",
    "recall    = true_positive/( true_positive + false_negative)\n",
    "            \n",
    "print (cnt , total , cnt/total)\n",
    "print ('precision = ' , precision , ' recall = ' , recall )\n",
    "print ( 'F1 score = ' , 2*((precision*recall)/ (precision+recall))  )\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done model fitting\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Naive bayes using cosine_values as embeddings\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "gnb_cosine = GaussianNB()\n",
    "NB_model_cosine = gnb_cosine.fit(cosine_similarity_words_features , label)\n",
    "gnb_concat = GaussianNB()\n",
    "NB_model_concat = gnb_concat.fit(concatinated_word_embeddings , label)\n",
    "\n",
    "print(\"Done model fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267 2721 0.46563763322307977\n",
      "precision =  0.46885694729637234  recall =  0.5025678650036683\n",
      "F1 score =  0.48512747875354106\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using Naive Baise model using cosine values as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "true_positive  = 0\n",
    "true_negative  = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        \n",
    "        cosine_similarity_between_w1_atb = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        cosine_similarity_between_w2_atb = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        \n",
    "        predicted_class = NB_model_cosine.predict( [cosine_similarity_between_w1_atb , cosine_similarity_between_w2_atb] )\n",
    "        \n",
    "        #print(predicted_class , row[3])\n",
    "        \n",
    "        if(predicted_class[0] == int(row[3])):\n",
    "            cnt += 1\n",
    "        if(int(row[3]) == 1):\n",
    "            if(predicted_class[0]==1):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        else:\n",
    "            if(predicted_class[0]==1):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        \n",
    "precision = true_positive/( true_positive + false_positive)\n",
    "recall    = true_positive/( true_positive + false_negative)\n",
    "            \n",
    "print (cnt , total , cnt/total)\n",
    "print ('precision = ', precision , ' recall = ' , recall )\n",
    "print ( 'F1 score = ' , 2*((precision*recall)/ (precision+recall))  )\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1282 2721 0.47115031238515254\n",
      "precision =  0.46  recall =  0.3206162876008804\n",
      "F1 score =  0.3778642455685257\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "#validate using Naive Bayes model using concatinating embedding as features vector\n",
    "\n",
    "total =0\n",
    "cnt = 0\n",
    "total =0\n",
    "cnt = 0\n",
    "true_positive  = 0\n",
    "true_negative  = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        concatinated_list = (model[row[0]]  +  model[row[1]]  + model[row[2]] ).tolist()\n",
    "        predicted_class = NB_model_concat.predict(concatinated_list)\n",
    "        \n",
    "        #print( predicted_class , row[3])\n",
    "        if(predicted_class[0]==int(row[3])):\n",
    "            cnt += 1\n",
    "        \n",
    "        if(int(row[3]) == 1):\n",
    "            if(predicted_class[0]==1):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        else:\n",
    "            if(predicted_class[0]==1):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        \n",
    "precision = true_positive/( true_positive + false_positive)\n",
    "recall    = true_positive/( true_positive + false_negative)\n",
    "            \n",
    "print (cnt , total , cnt/total)\n",
    "print ('precision = ',  precision , ' recall = ' ,recall )\n",
    "print ( 'F1 score = ' , 2*((precision*recall)/ (precision+recall))  )\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional feature extraction using wordnet\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def create_definition(word):\n",
    "    sys = wordnet.synsets(word)\n",
    "    definition_list = []\n",
    "    for x in sys:\n",
    "        line = x.definition().split() \n",
    "        definition_list += [w for w in line if not w in stop_words]\n",
    "    words_in_definition = re.sub(r'[^a-zA-Z0-9 ]',r'',\" \".join(definition_list))\n",
    "    return words_in_definition\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#Modified feature vectors\n",
    "\n",
    "total = 0\n",
    "feature_mat = []\n",
    "label = []\n",
    "with open('train.txt') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    for row in reader:\n",
    "        total += 1\n",
    "        \n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            cnt += 1\n",
    "            continue\n",
    "        w1_definition  = create_definition(row[0]).split()\n",
    "        w2_definition  = create_definition(row[1]).split()\n",
    "        att_definition = create_definition(row[2]).split()\n",
    "        #print(w1_definition , w2_definition , att_definition)\n",
    "        a=b=c=d=0.0\n",
    "        if(row[2] in w1_definition):\n",
    "            a = 1.0\n",
    "        if(row[2] in w2_definition):\n",
    "            b = 1.0\n",
    "        if(row[0] in att_definition):\n",
    "            c = 1.0\n",
    "        if(row[1] in att_definition):\n",
    "            d = 1.0\n",
    "        e = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        f = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        label.append(int(row[3]))\n",
    "        X = model[row[0]]\n",
    "        Y = model[row[1]]\n",
    "        Z = model[row[2]]\n",
    "        temp_list = np.concatenate((X,Y,Z , X-Z ,Y-Z )).tolist()\n",
    "        feature_mat.append( [a,b,c,d,e,f]+temp_list )\n",
    "        \n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n"
     ]
    }
   ],
   "source": [
    "#check feature vector\n",
    "X = model['sugar']\n",
    "Y = model['tea']\n",
    "Z = model['water']\n",
    "P = feature_mat[0]\n",
    "#print ( X, Y , np.concatenate((X,Y)) )\n",
    "Q = np.concatenate((X,Y,Z , X-Z ,Y-Z )).tolist()\n",
    "print (len((P+Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting\n"
     ]
    }
   ],
   "source": [
    "#train logistic regression model by using cosine values as feature vector\n",
    "\n",
    "logistic_regr_model_feature_extract = LogisticRegression()\n",
    "logistic_regr_model_feature_extract.fit( feature_mat , label )\n",
    "\n",
    "print(\"Done fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521 2721 0.5589856670341786\n",
      "precision =  0.5612321562734786  recall =  0.5480557593543653\n",
      "F1 score =  0.55456570155902\n",
      "done validating\n"
     ]
    }
   ],
   "source": [
    "total =0\n",
    "cnt = 0\n",
    "true_positive  = 0\n",
    "true_negative  = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "with open('validation.txt') as validation_file:\n",
    "    reader = csv.reader(validation_file)\n",
    "    for row in reader :\n",
    "        if (row[0] not in vocab) or (row[1] not in vocab) or (row[2] not in vocab):\n",
    "            continue\n",
    "        total += 1\n",
    "        w1_definition  = create_definition(row[0]).split()\n",
    "        w2_definition  = create_definition(row[1]).split()\n",
    "        att_definition = create_definition(row[2]).split()\n",
    "        #print(w1_definition , w2_definition , att_definition)\n",
    "        a=b=c=d=0\n",
    "        if(row[2] in w1_definition):\n",
    "            a = 1\n",
    "        if(row[2] in w2_definition):\n",
    "            b = 1\n",
    "        if(row[0] in att_definition):\n",
    "            c = 1\n",
    "        if(row[1] in att_definition):\n",
    "            d = 1\n",
    "        e = calculate_cosine_similartiy(row[0] , row[2])\n",
    "        f = calculate_cosine_similartiy(row[1] , row[2])\n",
    "        label.append(int(row[3]))\n",
    "        X = model[row[0]]\n",
    "        Y = model[row[1]]\n",
    "        Z = model[row[2]]\n",
    "        temp_list = np.concatenate((X,Y,Z , X-Z ,Y-Z )).tolist()\n",
    "        predicted_class = logistic_regr_model_feature_extract.predict([a,b,c,d,e,f]+temp_list)\n",
    "        \n",
    "        #print( predicted_class , row[3])\n",
    "        if(predicted_class[0]==int(row[3])):\n",
    "            cnt += 1\n",
    "        if(int(row[3]) == 1):\n",
    "            if(predicted_class[0]==1):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        else:\n",
    "            if(predicted_class[0]==1):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        \n",
    "precision = true_positive/( true_positive + false_positive)\n",
    "recall    = true_positive/( true_positive + false_negative)\n",
    "            \n",
    "print (cnt , total , cnt/total)\n",
    "print ('precision = ' , precision , ' recall = ' , recall )\n",
    "print ( 'F1 score = ' , 2*((precision*recall)/ (precision+recall))  )\n",
    "print('done validating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
